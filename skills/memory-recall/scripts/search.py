#!/usr/bin/env python3
"""
è®°å¿†æ£€ç´¢è„šæœ¬
æ”¯æŒè¯­ä¹‰æœç´¢å†å²å¯¹è¯å’Œç»éªŒæ²‰æ·€ï¼Œä½¿ç”¨æœ¬åœ° embedding æ¨¡å‹ã€‚

ç”¨æ³•:
    # è¯­ä¹‰æœç´¢
    python3 search.py --query "å¯¹è¯å½’æ¡£æ€ä¹ˆå®ç°çš„" --top 5
    
    # æŸ¥çœ‹åŸæ–‡
    python3 search.py --show 1
    
    # æŒ‰æ¡ä»¶è¿‡æ»¤
    python3 search.py --query "å…³é”®è¯" --type conversation --date-range "2026-01-01,2026-01-18"
"""

import argparse
import json
import os
import sqlite3
import sys
from pathlib import Path


# è·¯å¾„é…ç½®
DB_PATH = Path.home() / '.gemini' / 'memory' / 'conversations.db'
SKILLS_DIR = Path.home() / '.gemini' / 'antigravity' / 'skills'
MODEL_DIR = Path.home() / '.gemini' / 'models' / 'all-MiniLM-L6-v2'

# å…¨å±€å˜é‡
_model = None
_use_semantic = False


def check_dependencies():
    """æ£€æŸ¥ä¾èµ–æ˜¯å¦å·²å®‰è£…ï¼Œç¼ºå¤±æ—¶è¯¢é—®ç”¨æˆ·æ˜¯å¦å®‰è£…"""
    global _use_semantic
    try:
        from sentence_transformers import SentenceTransformer
        _use_semantic = True
        return True
    except ImportError:
        print("â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”")
        print("â”‚ ğŸ” è¯­ä¹‰æœç´¢ä¾èµ–æœªå®‰è£…                              â”‚")
        print("â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤")
        print("â”‚ è¯­ä¹‰æœç´¢å¯ä»¥æ›´æ™ºèƒ½åœ°ç†è§£æ‚¨çš„é—®é¢˜ã€‚                 â”‚")
        print("â”‚ é¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆçº¦ 80MBï¼‰ï¼Œå¯èƒ½éœ€è¦å‡ åˆ†é’Ÿã€‚  â”‚")
        print("â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜")
        print()
        print("æ˜¯å¦å®‰è£…è¯­ä¹‰æœç´¢ä¾èµ–ï¼Ÿ")
        print("  è¾“å…¥ y å®‰è£…ï¼ˆæ¨èï¼‰")
        print("  è¾“å…¥ n ä½¿ç”¨å…³é”®è¯æœç´¢ï¼ˆåŠŸèƒ½å—é™ï¼‰")
        print()
        
        try:
            choice = input("è¯·é€‰æ‹© [y/n]: ").strip().lower()
        except EOFError:
            # éäº¤äº’æ¨¡å¼ï¼Œé™çº§åˆ°å…³é”®è¯æœç´¢
            choice = 'n'
        
        if choice == 'y':
            print()
            print("æ­£åœ¨å®‰è£… sentence-transformers...")
            import subprocess
            result = subprocess.run(
                [sys.executable, '-m', 'pip', 'install', 'sentence-transformers'],
                capture_output=True,
                text=True
            )
            if result.returncode == 0:
                print("âœ… å®‰è£…æˆåŠŸï¼")
                print()
                # é‡æ–°å¯¼å…¥
                from sentence_transformers import SentenceTransformer
                _use_semantic = True
                return True
            else:
                print("âŒ å®‰è£…å¤±è´¥ï¼Œå°†ä½¿ç”¨å…³é”®è¯æœç´¢ã€‚")
                print(f"é”™è¯¯ä¿¡æ¯: {result.stderr[:200]}")
                _use_semantic = False
                return False
        else:
            print()
            print("å·²é€‰æ‹©å…³é”®è¯æœç´¢æ¨¡å¼ã€‚")
            _use_semantic = False
            return False


def load_model():
    """åŠ è½½æˆ–ä¸‹è½½ embedding æ¨¡å‹"""
    global _model
    if _model is not None:
        return _model
    
    if not _use_semantic:
        return None
    
    from sentence_transformers import SentenceTransformer
    
    if MODEL_DIR.exists():
        print("åŠ è½½æœ¬åœ°æ¨¡å‹...")
        _model = SentenceTransformer(str(MODEL_DIR))
    else:
        print("é¦–æ¬¡è¿è¡Œï¼Œæ­£åœ¨ä¸‹è½½è¯­ä¹‰æœç´¢æ¨¡å‹ï¼ˆçº¦ 80MBï¼‰...")
        _model = SentenceTransformer('all-MiniLM-L6-v2')
        MODEL_DIR.parent.mkdir(parents=True, exist_ok=True)
        _model.save(str(MODEL_DIR))
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {MODEL_DIR}")
    
    return _model


def get_embedding(text: str):
    """ç”Ÿæˆæ–‡æœ¬çš„ embedding"""
    model = load_model()
    if model is None:
        return None
    return model.encode(text)


def cosine_similarity(a, b):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    import numpy as np
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))


def search_conversations_keyword(keyword: str, date_range: str = None, limit: int = 10):
    """å…³é”®è¯æœç´¢å¯¹è¯"""
    if not DB_PATH.exists():
        return []
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    query = """
        SELECT id, title, archive_time, turn_count, file_path, first_message, project_path
        FROM conversations
        WHERE (title LIKE ? OR first_message LIKE ?)
    """
    params = [f'%{keyword}%', f'%{keyword}%']
    
    if date_range:
        start_date, end_date = date_range.split(',')
        query += " AND archive_time BETWEEN ? AND ?"
        params.extend([start_date, end_date + ' 23:59'])
    
    query += " ORDER BY archive_time DESC LIMIT ?"
    params.append(limit)
    
    cursor.execute(query, params)
    results = cursor.fetchall()
    conn.close()
    
    return [
        {
            'id': r[0],
            'title': r[1],
            'time': r[2],
            'turns': r[3],
            'file_path': r[4],
            'first_message': r[5],
            'project_path': r[6],
            'type': 'conversation',
            'score': 1.0  # å…³é”®è¯åŒ¹é…é»˜è®¤åˆ†æ•°
        }
        for r in results
    ]


def search_skills_keyword(keyword: str):
    """å…³é”®è¯æœç´¢æŠ€èƒ½"""
    results = []
    keyword_lower = keyword.lower()
    
    # æœç´¢å…¨å±€æŠ€èƒ½
    if SKILLS_DIR.exists():
        for skill_dir in SKILLS_DIR.iterdir():
            if not skill_dir.is_dir():
                continue
            skill_file = skill_dir / 'SKILL.md'
            if not skill_file.exists():
                continue
            
            content = skill_file.read_text(encoding='utf-8')
            
            # æå– name å’Œ description
            name = skill_dir.name
            description = ""
            if 'description:' in content:
                desc_start = content.find('description:')
                desc_end = content.find('---', desc_start + 1)
                if desc_end > desc_start:
                    description = content[desc_start:desc_end]
            
            if keyword_lower in name.lower() or keyword_lower in description.lower():
                results.append({
                    'id': f'skill:{name}',
                    'title': name,
                    'time': '',
                    'turns': 0,
                    'file_path': str(skill_file),
                    'first_message': description[:100],
                    'project_path': '',
                    'type': 'skill',
                    'score': 1.0
                })
    
    return results


def search_semantic(query: str, type_filter: str = None, date_range: str = None, limit: int = 10):
    """è¯­ä¹‰æœç´¢ï¼ˆéœ€è¦ embeddings æ”¯æŒï¼‰"""
    # ç›®å‰å›é€€åˆ°å…³é”®è¯æœç´¢
    # TODO: å®ç°çœŸæ­£çš„è¯­ä¹‰æœç´¢
    results = []
    
    if type_filter != 'skill':
        results.extend(search_conversations_keyword(query, date_range, limit))
    
    if type_filter != 'conversation':
        results.extend(search_skills_keyword(query))
    
    # æŒ‰ç›¸å…³åº¦æ’åº
    results.sort(key=lambda x: x['score'], reverse=True)
    
    return results[:limit]


def display_results(results):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not results:
        print("æœªæ‰¾åˆ°ç›¸å…³è®°å½•")
        return
    
    print(f"æ‰¾åˆ° {len(results)} æ¡ç›¸å…³è®°å½•:\n")
    
    for i, r in enumerate(results, 1):
        type_label = "å¯¹è¯å½’æ¡£" if r['type'] == 'conversation' else "æŠ€èƒ½æ–‡ä»¶"
        print(f"[{i}] {r['title']} ({r['time'] or 'æŠ€èƒ½'})")
        first_msg = r['first_message'][:50] + '...' if len(r['first_message']) > 50 else r['first_message']
        print(f"    é¦–å¥: {first_msg}")
        print(f"    ç›¸å…³åº¦: {r['score']:.2f}")
        print(f"    ç±»å‹: {type_label}")
        print(f"    æ–‡ä»¶: {r['file_path']}")
        print()


def show_content(item_id: int):
    """æ˜¾ç¤ºåŸæ–‡å†…å®¹"""
    # æŸ¥æ‰¾å¯¹åº”è®°å½•
    if not DB_PATH.exists():
        print("æ•°æ®åº“ä¸å­˜åœ¨")
        return
    
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute("""
        SELECT title, file_path FROM conversations WHERE id = ?
    """, (item_id,))
    
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        print(f"æœªæ‰¾åˆ° ID ä¸º {item_id} çš„è®°å½•")
        return
    
    title, file_path = result
    
    if not Path(file_path).exists():
        print(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return
    
    print(f"=== {title} ===\n")
    print(Path(file_path).read_text(encoding='utf-8'))


def main():
    parser = argparse.ArgumentParser(description='è®°å¿†æ£€ç´¢')
    parser.add_argument('--query', '-q', help='æœç´¢å…³é”®è¯æˆ–é—®é¢˜')
    parser.add_argument('--show', '-s', type=int, help='æ˜¾ç¤ºæŒ‡å®š ID çš„åŸæ–‡')
    parser.add_argument('--top', '-n', type=int, default=5, help='è¿”å›ç»“æœæ•°é‡')
    parser.add_argument('--type', '-t', choices=['conversation', 'skill'], help='è¿‡æ»¤ç±»å‹')
    parser.add_argument('--date-range', help='æ—¥æœŸèŒƒå›´ï¼Œæ ¼å¼: å¼€å§‹,ç»“æŸ')
    parser.add_argument('--list', '-l', action='store_true', help='åˆ—å‡ºæœ€è¿‘è®°å½•')
    args = parser.parse_args()
    
    # æ£€æŸ¥ä¾èµ–
    check_dependencies()
    
    if args.show:
        show_content(args.show)
    elif args.query:
        results = search_semantic(args.query, args.type, args.date_range, args.top)
        display_results(results)
    elif args.list:
        results = search_conversations_keyword('', args.date_range, args.top)
        display_results(results)
    else:
        parser.print_help()


if __name__ == '__main__':
    main()
